{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3c8b1249",
      "metadata": {
        "id": "3c8b1249"
      },
      "source": [
        "\n",
        "# Hybrid CNN‚ÄìGNN for OCT (v4): GATv2 + Class-Specific Attention + EMA + Pseudo-Mask Segmentation\n",
        "\n",
        "**What‚Äôs new vs v3**  \n",
        "- **GATv2-style GNN** with **relative-position bias** on the 4-neighbor grid ‚Üí sharper context propagation.  \n",
        "- **Class-specific attention** (C attention maps) instead of class-agnostic ‚Üí clearer per-class localization.  \n",
        "- **EMA (Exponential Moving Average) teacher** of model parameters ‚Üí stabler attention & pseudo masks at test time.  \n",
        "- Keeps **pseudo-mask training with partial CE + Dice**, **edge-aware TV**, **adversarial erasing**, **sufficiency/necessity**, and **contrastive patches**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31c95fea",
      "metadata": {
        "id": "31c95fea"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title ‚¨áÔ∏è Install\n",
        "# %%capture\n",
        "!pip -q install --upgrade pip\n",
        "!pip -q install albumentations==1.3.1 opencv-python-headless==4.10.0.84\n",
        "!pip -q install torchmetrics==1.4.0.post0 pytorch-lightning==2.4.0 rich==13.7.1 kagglehub==0.2.7\n",
        "!pip -q install tensorboard grad-cam==1.5.5\n",
        "\n",
        "import torch, math, numpy as np\n",
        "print(\"PyTorch:\", torch.__version__, \"| CUDA:\", torch.version.cuda)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f275cb47",
      "metadata": {
        "id": "f275cb47"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title üì¶ Imports & Config\n",
        "import os, random, math\n",
        "from dataclasses import dataclass, asdict\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, Tuple, Optional\n",
        "\n",
        "import numpy as np, cv2, albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from torchmetrics import Accuracy, Precision, Recall, F1Score, ConfusionMatrix\n",
        "from torchvision import models\n",
        "from rich.console import Console\n",
        "from rich.table import Table\n",
        "\n",
        "console = Console()\n",
        "SEED=42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "CLASS_NAMES = [\"CNV\",\"DME\",\"DRUSEN\",\"NORMAL\"]\n",
        "CLASS_TO_IDX = {c:i for i,c in enumerate(CLASS_NAMES)}\n",
        "NUM_CLASSES = len(CLASS_NAMES)\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    data_root: str = \"\"\n",
        "    train_dirname: str = \"training/train\"\n",
        "    val_dirname: str = \"training/val\"\n",
        "    test_dirname: str = \"test\"\n",
        "\n",
        "    img_size: int = 256\n",
        "    normalize_mean: float = 0.5\n",
        "    normalize_std: float = 0.5\n",
        "\n",
        "    # Augs\n",
        "    aug_rotation: int = 10\n",
        "    aug_hflip_p: float = 0.5\n",
        "    aug_brightness: float = 0.2\n",
        "    aug_contrast: float = 0.2\n",
        "\n",
        "    # Train\n",
        "    batch_size: int = 16\n",
        "    num_workers: int = 2\n",
        "    max_epochs: int = 20\n",
        "    lr: float = 2e-4\n",
        "    weight_decay: float = 1e-4\n",
        "    precision: str = \"16-mixed\"\n",
        "\n",
        "    # Graph\n",
        "    patch_rows: int = 16\n",
        "    patch_cols: int = 16\n",
        "    gnn_hidden: int = 256\n",
        "    gnn_layers: int = 2\n",
        "    gat_heads: int = 4\n",
        "\n",
        "    # Weak loc\n",
        "    att_temperature: float = 0.5\n",
        "    lambda_entropy: float = 0.02\n",
        "    lambda_tv: float = 0.02\n",
        "    lambda_diverse: float = 0.15\n",
        "    erase_topk_frac: float = 0.15\n",
        "\n",
        "    # Pseudo-mask\n",
        "    pm_high_q: float = 0.85\n",
        "    pm_low_q: float = 0.4\n",
        "    lambda_seg_ce: float = 0.7\n",
        "    lambda_seg_dice: float = 0.7\n",
        "    edge_tv_weight: float = 0.02\n",
        "\n",
        "    # Contrastive, suff/nec\n",
        "    topk_patches: int = 2\n",
        "    lambda_contrast: float = 0.2\n",
        "    contrast_temperature: float = 0.07\n",
        "    suff_topk_frac: float = 0.10\n",
        "    nece_topk_frac: float = 0.10\n",
        "    lambda_suff: float = 0.6\n",
        "    lambda_nece: float = 0.6\n",
        "    nece_margin: float = 0.3\n",
        "\n",
        "    # EMA\n",
        "    ema_decay: float = 0.999\n",
        "\n",
        "    # Logging\n",
        "    project: str = \"oct_weakloc_v4\"\n",
        "    run_name: str = \"r34_gatv2_milclass_ema_pseudomask\"\n",
        "    save_top_k: int = 2\n",
        "    monitor: str = \"val/cls_f1\"\n",
        "    monitor_mode: str = \"max\"\n",
        "\n",
        "cfg = Config()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8275e8d8",
      "metadata": {
        "id": "8275e8d8"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title üì• KaggleHub dataset\n",
        "import kagglehub\n",
        "from pathlib import Path\n",
        "\n",
        "path = kagglehub.dataset_download(\"fabrizioravelli/retinal-oct-images-splitted\")\n",
        "print(\"Dataset downloaded to:\", path)\n",
        "\n",
        "p = Path(path); root=None\n",
        "for cand in p.rglob(\"training\"):\n",
        "    base=cand.parent\n",
        "    if (base/\"training\"/\"train\").exists() and (base/\"training\"/\"val\").exists() and (base/\"test\").exists():\n",
        "        root=base; break\n",
        "if root is None:\n",
        "    for base in p.rglob(\"*\"):\n",
        "        if (base/\"train\").exists() and (base/\"val\").exists() and (base/\"test\").exists():\n",
        "            cfg.train_dirname=\"train\"; cfg.val_dirname=\"val\"; cfg.test_dirname=\"test\"; root=base; break\n",
        "cfg.data_root=str(root) if root else \"\"\n",
        "print(\"Using data_root:\", cfg.data_root)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fdf8798",
      "metadata": {
        "id": "9fdf8798"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title üßπ Data\n",
        "def build_transforms(is_train: bool):\n",
        "    t = []\n",
        "    if is_train:\n",
        "        t += [A.HorizontalFlip(p=cfg.aug_hflip_p),\n",
        "              A.Rotate(limit=cfg.aug_rotation, border_mode=cv2.BORDER_REFLECT_101, p=0.7),\n",
        "              A.ColorJitter(brightness=cfg.aug_brightness, contrast=cfg.aug_contrast, saturation=0, hue=0, p=0.5)]\n",
        "    t += [A.Resize(cfg.img_size, cfg.img_size, interpolation=cv2.INTER_AREA),\n",
        "          A.Normalize(mean=(cfg.normalize_mean,), std=(cfg.normalize_std,)),\n",
        "          ToTensorV2()]\n",
        "    return A.Compose(t)\n",
        "\n",
        "def _read_gray(path:str):\n",
        "    img=cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
        "    if img is None: raise FileNotFoundError(path)\n",
        "    return img\n",
        "\n",
        "class OCTClsDataset(Dataset):\n",
        "    def __init__(self, root:str, split:str):\n",
        "        self.root = Path(root)/split\n",
        "        self.samples=[]\n",
        "        for cls in CLASS_NAMES:\n",
        "            d=self.root/cls\n",
        "            if not d.exists(): continue\n",
        "            for p in d.glob(\"*\"):\n",
        "                if p.suffix.lower() in {\".png\",\".jpg\",\".jpeg\",\".bmp\",\".tif\",\".tiff\"}:\n",
        "                    self.samples.append((p.as_posix(), CLASS_TO_IDX[cls]))\n",
        "        self.tfs = build_transforms(is_train=(split.endswith(\"train\") or split==\"train\"))\n",
        "    def __len__(self): return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        path,y = self.samples[idx]\n",
        "        img=_read_gray(path)\n",
        "        out=self.tfs(image=img)\n",
        "        x=out[\"image\"]\n",
        "        return {\"image\":x, \"label\":torch.tensor(y), \"path\":path}\n",
        "\n",
        "class OCTDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, cfg):\n",
        "      super().__init__()\n",
        "      self._log_hyperparams = False;\n",
        "      self.cfg=cfg\n",
        "    def setup(self, stage=None):\n",
        "        self.ds_train=OCTClsDataset(self.cfg.data_root, self.cfg.train_dirname)\n",
        "        self.ds_val=OCTClsDataset(self.cfg.data_root, self.cfg.val_dirname)\n",
        "        self.ds_test=OCTClsDataset(self.cfg.data_root, self.cfg.test_dirname)\n",
        "    def train_dataloader(self): return DataLoader(self.ds_train, batch_size=cfg.batch_size, shuffle=True, num_workers=cfg.num_workers, pin_memory=True, drop_last=True)\n",
        "    def val_dataloader(self): return DataLoader(self.ds_val, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers, pin_memory=True)\n",
        "    def test_dataloader(self): return DataLoader(self.ds_test, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers, pin_memory=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4512e095",
      "metadata": {
        "id": "4512e095",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title üß† GATv2 GNN with relative-position bias + Model\n",
        "def build_grid(pr:int, pc:int, device):\n",
        "    n=pr*pc\n",
        "    A=torch.zeros((n,n), dtype=torch.float32, device=device)\n",
        "    coords=torch.zeros((n,2), dtype=torch.float32, device=device)\n",
        "    def idx(r,c): return r*pc+c\n",
        "    for r in range(pr):\n",
        "        for c in range(pc):\n",
        "            u=idx(r,c); coords[u]=torch.tensor([r/float(pr-1), c/float(pc-1)], device=device)\n",
        "            A[u,u]=1.0\n",
        "            if r>0: A[u, idx(r-1,c)]=1.0\n",
        "            if r<pr-1: A[u, idx(r+1,c)]=1.0\n",
        "            if c>0: A[u, idx(r,c-1)]=1.0\n",
        "            if c<pc-1: A[u, idx(r,c+1)]=1.0\n",
        "    deg=A.sum(1, keepdim=True)+1e-6\n",
        "    A_hat=A/deg\n",
        "    return A_hat, coords\n",
        "\n",
        "class GATv2Layer(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, heads=4, pos_dim=2):\n",
        "        super().__init__()\n",
        "        self.heads=heads\n",
        "        # project input to (out_ch * heads) features\n",
        "        self.out_ch = out_ch * heads\n",
        "        self.W      = nn.Linear(in_ch, self.out_ch, bias=False)\n",
        "\n",
        "        # attention MLP now needs 2*self.out_ch + pos_dim inputs\n",
        "        self.a      = nn.Linear(self.out_ch*2 + pos_dim, 1, bias=False)\n",
        "        self.leaky  = nn.LeakyReLU(0.2)\n",
        "        self.norm   = nn.LayerNorm(self.out_ch)\n",
        "    def forward(self, X, A_hat, coords):\n",
        "        B,N,D = X.shape\n",
        "        Wh = self.W(X)  # [B,N,out*H]\n",
        "        Wh_i = Wh.unsqueeze(2).expand(B,N,N,self.out_ch)\n",
        "        Wh_j = Wh.unsqueeze(1).expand(B,N,N,self.out_ch)\n",
        "        dpos = (coords.unsqueeze(1) - coords.unsqueeze(0))  # [N,N,2]\n",
        "        dpos = dpos.unsqueeze(0).expand(B,-1,-1,-1)\n",
        "        att_in = torch.cat([Wh_i, Wh_j, dpos], dim=-1)\n",
        "        # e = self.leaky(self.a(att_in).squeeze(-1))\n",
        "        # neg_inf = torch.full_like(e, -1e9)\n",
        "        e = self.leaky(self.a(att_in).squeeze(-1))\n",
        "        neg_inf = torch.full_like(e, torch.finfo(e.dtype).min)\n",
        "        e = torch.where(A_hat.unsqueeze(0)>0, e, neg_inf)\n",
        "        alpha = torch.softmax(e, dim=-1)\n",
        "        H = torch.matmul(alpha, Wh)\n",
        "        return self.norm(F.relu(H))\n",
        "\n",
        "class GATBlock(nn.Module):\n",
        "    def __init__(self, in_ch, hidden, layers=2, heads=4):\n",
        "        super().__init__()\n",
        "        L=[]; ch=in_ch\n",
        "        for _ in range(layers):\n",
        "            L.append(GATv2Layer(ch, hidden//heads, heads=heads)); ch=(hidden//heads)*heads\n",
        "        self.net=nn.ModuleList(L); self.out_dim=ch\n",
        "    def forward(self, X, A_hat, coords):\n",
        "        for m in self.net: X=m(X,A_hat,coords)\n",
        "        return X\n",
        "\n",
        "class ResNet34Encoder(nn.Module):\n",
        "    def __init__(self, pretrained=True):\n",
        "        super().__init__()\n",
        "        m=models.resnet34(weights=models.ResNet34_Weights.DEFAULT if pretrained else None)\n",
        "        w=m.conv1.weight.data\n",
        "        m.conv1 = nn.Conv2d(1,64,7,2,3,bias=False)\n",
        "        if pretrained: m.conv1.weight.data = w.mean(1, keepdim=True)\n",
        "        self.stem=nn.Sequential(m.conv1, m.bn1, m.relu, m.maxpool)\n",
        "        self.layer1=m.layer1; self.layer2=m.layer2; self.layer3=m.layer3; self.layer4=m.layer4\n",
        "    def forward(self,x):\n",
        "        x=self.stem(x); x1=self.layer1(x); x2=self.layer2(x1); x3=self.layer3(x2); x4=self.layer4(x3)\n",
        "        return x2, x3\n",
        "\n",
        "class ClassSpecificMIL(nn.Module):\n",
        "    def __init__(self, in_ch, num_classes, temperature=1.0):\n",
        "        super().__init__(); self.t=temperature\n",
        "        self.fc = nn.Linear(in_ch, num_classes)\n",
        "        self.att_proj = nn.Sequential(nn.Linear(in_ch, in_ch//2), nn.Tanh(), nn.Linear(in_ch//2, num_classes))\n",
        "    def forward(self, X, mask=None, temperature=None):\n",
        "        tau = temperature if temperature is not None else self.t\n",
        "        node_scores = self.fc(X)                      # [B,N,C]\n",
        "        att_scores = self.att_proj(X) / max(tau,1e-4) # [B,N,C]\n",
        "        if mask is not None:\n",
        "          # use dtype‚Äôs min rather than a hard -1e9 constant\n",
        "          neg_inf = torch.full_like(att_scores, torch.finfo(att_scores.dtype).min)\n",
        "          mask3 = mask.unsqueeze(-1).expand_as(att_scores)\n",
        "          att_scores = torch.where(mask3 > 0.5, att_scores, neg_inf)\n",
        "\n",
        "        A = torch.softmax(att_scores, dim=1)          # [B,N,C]\n",
        "        logits = (A * node_scores).sum(dim=1)         # [B,C]\n",
        "        return logits, A.transpose(1,2)               # [B,C,N]\n",
        "\n",
        "class SmallDecoder(nn.Module):\n",
        "    def __init__(self, ch2=128, ch3=256, out_ch=NUM_CLASSES, use_prior=True):\n",
        "        super().__init__()\n",
        "        self.use_prior = use_prior\n",
        "        prior_ch = NUM_CLASSES if use_prior else 0\n",
        "\n",
        "        self.up3 = nn.Sequential(\n",
        "            nn.Conv2d(ch3, 128, 3, padding=1), nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
        "        )\n",
        "        self.fuse = nn.Sequential(\n",
        "            nn.Conv2d(128 + 128 + prior_ch, 128, 3, padding=1), nn.ReLU()\n",
        "        )\n",
        "        self.up2 = nn.Sequential(\n",
        "            nn.Conv2d(128, 64, 3, padding=1), nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
        "        )\n",
        "        self.up1 = nn.Sequential(\n",
        "            nn.Conv2d(64, 32, 3, padding=1), nn.ReLU(),\n",
        "            nn.Upsample(scale_factor=4, mode='bilinear', align_corners=False)\n",
        "        )\n",
        "        self.out = nn.Conv2d(32, out_ch, 1)\n",
        "\n",
        "    def forward(self, x2, x3, prior=None):\n",
        "        u3 = self.up3(x3)\n",
        "        if self.use_prior and prior is not None:\n",
        "            f = self.fuse(torch.cat([u3, x2, prior], dim=1))\n",
        "        else:\n",
        "            f = self.fuse(torch.cat([u3, x2], dim=1))\n",
        "        u2 = self.up2(f)\n",
        "        u1 = self.up1(u2)\n",
        "        return self.out(u1)\n",
        "\n",
        "\n",
        "class HybridV4(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__(); self.cfg=cfg\n",
        "        self.enc=ResNet34Encoder(pretrained=True)\n",
        "        self.gnn=GATBlock(256, cfg.gnn_hidden, cfg.gnn_layers, heads=cfg.gat_heads)\n",
        "        self.proj=nn.Linear(self.gnn.out_dim, cfg.gnn_hidden)\n",
        "        self.mil=ClassSpecificMIL(cfg.gnn_hidden, NUM_CLASSES, temperature=cfg.att_temperature)\n",
        "        self.mil_erase=ClassSpecificMIL(cfg.gnn_hidden, NUM_CLASSES, temperature=cfg.att_temperature)\n",
        "        self.dec=SmallDecoder(128,256, NUM_CLASSES)\n",
        "        # localization head from encoder features\n",
        "        self.loc_head = nn.Conv2d(256, NUM_CLASSES, kernel_size=1, bias=True)\n",
        "        self.topk_ratio = getattr(cfg, \"loc_topk_ratio\", 0.2)\n",
        "        self.loc_logsumexp = getattr(cfg, \"loc_logsumexp\", False)\n",
        "\n",
        "    def _spatial_logits_to_image_logits(self, maps):\n",
        "        B, C, H, W = maps.shape\n",
        "        K = max(1, int(self.topk_ratio * H * W))\n",
        "        maps_flat = maps.view(B, C, -1)\n",
        "        if self.loc_logsumexp:\n",
        "            return torch.logsumexp(maps_flat, dim=-1) - math.log(H*W)\n",
        "        else:\n",
        "            topk_vals, _ = torch.topk(maps_flat, k=K, dim=-1)\n",
        "            return topk_vals.mean(dim=-1)\n",
        "\n",
        "\n",
        "    def forward(self, x, A_hat, coords):\n",
        "        x2, x3 = self.enc(x)\n",
        "\n",
        "        # --- GNN over x3 tokens ---\n",
        "        B, C, H16, W16 = x3.shape\n",
        "        # sanity: graph size must match token grid\n",
        "        assert self.cfg.patch_rows == H16 and self.cfg.patch_cols == W16, \\\n",
        "            f\"Graph grid ({self.cfg.patch_rows},{self.cfg.patch_cols}) must match x3 spatial ({H16},{W16}).\"\n",
        "\n",
        "        X = x3.flatten(2).transpose(1, 2)   # [B, N, 256] where N = H16*W16\n",
        "        Xg = self.gnn(X, A_hat, coords)     # [B, N, gnn_out]\n",
        "        Xg = self.proj(Xg)                  # [B, N, cfg.gnn_hidden]\n",
        "\n",
        "        # --- MIL branches ---\n",
        "        logits1, A1 = self.mil(Xg, None, self.cfg.att_temperature)  # [B,C], [B,C,N]\n",
        "        with torch.no_grad():\n",
        "            att_max = A1.max(dim=1).values                           # [B,N]\n",
        "            k = max(1, int(self.cfg.erase_topk_frac * Xg.size(1)))\n",
        "            top_idx = torch.topk(att_max, k, dim=1).indices\n",
        "            drop_mask = torch.ones_like(att_max)\n",
        "            drop_mask.scatter_(1, top_idx, 0.0)\n",
        "\n",
        "        logits2, A2 = self.mil_erase(Xg, drop_mask, self.cfg.att_temperature)  # [B,C], [B,C,N]\n",
        "\n",
        "        # --- Localization head from x3 ---\n",
        "        loc_maps = self.loc_head(x3)                                  # [B, C, H16, W16]\n",
        "        loc_img_logits = self._spatial_logits_to_image_logits(loc_maps)  # [B, C]\n",
        "\n",
        "        # --- Fuse classification signals ---\n",
        "        logits = 0.5 * (logits1 + logits2) + 0.5 * loc_img_logits     # [B, C]\n",
        "\n",
        "        # --- Decoder with prior ---\n",
        "        _, _, H8, W8 = x2.shape\n",
        "        prior_up = F.interpolate(loc_maps, size=(H8, W8), mode='bilinear', align_corners=False)\n",
        "        seg_logits = self.dec(x2, x3, prior=prior_up)                  # [B, C, H, W] (restored to /1 inside dec)\n",
        "\n",
        "        return (logits1, A1), (logits2, A2), loc_maps, logits, Xg, x2, x3, seg_logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d09d7c9c",
      "metadata": {
        "cellView": "form",
        "id": "d09d7c9c"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title üîß Losses & helpers\n",
        "def entropy_loss(att):\n",
        "    eps=1e-8; H=-(att*(att+eps).log()).sum(dim=-1)\n",
        "    Z=math.log(att.size(-1)+eps)\n",
        "    return (H/Z).mean()\n",
        "\n",
        "def tv_edge_loss(mask, img, weight=0.02):\n",
        "    gx = F.pad(img[:,:,:,1:]-img[:,:,:,:-1], (0,1,0,0))\n",
        "    gy = F.pad(img[:,:,1:,:]-img[:,:,:-1,:], (0,0,0,1))\n",
        "    wx = torch.exp(- (gx*10).abs()); wy = torch.exp(- (gy*10).abs())\n",
        "    mx = F.pad(mask[:,:,:,1:]-mask[:,:,:,:-1], (0,1,0,0)).abs()\n",
        "    my = F.pad(mask[:,:,1:,:]-mask[:,:,:-1,:], (0,0,0,1)).abs()\n",
        "    return weight*((wx*mx).mean() + (wy*my).mean())\n",
        "\n",
        "def partial_ce(pred, target, ignore_mask):\n",
        "    if ignore_mask.sum()==target.numel():\n",
        "        return pred.new_tensor(0.0)\n",
        "    loss = F.cross_entropy(pred.permute(0,2,3,1).reshape(-1, pred.size(1)),\n",
        "                           target.view(-1), reduction='none', ignore_index=255)\n",
        "    loss = loss.view(target.shape)\n",
        "    return loss[ignore_mask==0].mean() if (ignore_mask==0).any() else pred.new_tensor(0.0)\n",
        "\n",
        "def soft_dice_loss(logits, target_onehot, eps=1e-6):\n",
        "    probs = torch.softmax(logits, dim=1)\n",
        "    valid = target_onehot.sum(dim=1, keepdim=True)>0.5\n",
        "    if valid.sum()==0: return logits.new_tensor(0.0)\n",
        "    inter = (probs*target_onehot).sum(dim=(2,3))\n",
        "    union = (probs+target_onehot).sum(dim=(2,3))\n",
        "    dice = (2*inter+eps)/(union+eps)\n",
        "    return 1-dice.mean()\n",
        "\n",
        "def supcon(feats, labels, t):\n",
        "    if feats.size(0)<2: return feats.new_tensor(0.0)\n",
        "    sim=torch.matmul(feats,feats.t())/t\n",
        "    eye=torch.eye(sim.size(0), device=sim.device, dtype=torch.bool)\n",
        "    same=(labels.view(-1,1)==labels.view(1,-1)); pos=same & (~eye)\n",
        "    sim_exp=torch.exp(sim) * (~eye); denom=sim_exp.sum(1, keepdim=True)+1e-8\n",
        "    num=(sim_exp*pos).sum(1, keepdim=True)+1e-8\n",
        "    valid=(pos.sum(1)>0).float().view(-1,1)\n",
        "    loss = -torch.log(num/denom)*valid\n",
        "    return loss.sum()/valid.sum().clamp_min(1.0)\n",
        "\n",
        "# --- ADD THIS BELOW YOUR HELPERS ---\n",
        "import math\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def _pool_loc_maps_to_logits(loc_maps, loc_topk_ratio=0.2, use_lse=False):\n",
        "    \"\"\"\n",
        "    loc_maps: [B, C, H, W]  ->  [B, C]\n",
        "    \"\"\"\n",
        "    B, C, H, W = loc_maps.shape\n",
        "    K = max(1, int(loc_topk_ratio * H * W))\n",
        "    flat = loc_maps.view(B, C, -1)  # [B,C,HW]\n",
        "    if use_lse:\n",
        "        return torch.logsumexp(flat, dim=-1) - math.log(H * W)\n",
        "    topk_vals, _ = torch.topk(flat, k=K, dim=-1)\n",
        "    return topk_vals.mean(dim=-1)\n",
        "\n",
        "def compute_total_loss(\n",
        "    model_outputs,\n",
        "    batch,\n",
        "    cfg,\n",
        "    num_classes=None,\n",
        "    use_masks=True,\n",
        "):\n",
        "    \"\"\"\n",
        "    model_outputs expected from HybridV4.forward:\n",
        "      (logits1, A1), (logits2, A2), loc_maps, logits, Xg, x2, x3, seg_logits\n",
        "\n",
        "    batch must contain:\n",
        "      batch[\"image\"] : [B,1,H,W] (normalized)\n",
        "      batch[\"label\"] : [B]       (class indices, 0..C-1)\n",
        "      batch[\"mask\"]  : [B,H,W]   (optional; 0..C-1, 255=ignore)\n",
        "\n",
        "    Returns:\n",
        "      loss (scalar), logs (dict of individual terms)\n",
        "    \"\"\"\n",
        "    # Unpack\n",
        "    (logits1, A1), (logits2, A2), loc_maps, logits, Xg, x2, x3, seg_logits = model_outputs\n",
        "    imgs   = batch[\"image\"]\n",
        "    y_img  = batch[\"label\"]                    # [B]\n",
        "    has_gt_masks = use_masks and (\"mask\" in batch) and (batch[\"mask\"] is not None)\n",
        "    y_mask = batch.get(\"mask\", None)\n",
        "\n",
        "    B = y_img.size(0)\n",
        "    device = imgs.device\n",
        "    if num_classes is None:\n",
        "        num_classes = logits.size(1)\n",
        "\n",
        "    # ---- Weights (override via cfg if you like) ----\n",
        "    w_ce_cls   = getattr(cfg, \"w_ce_cls\",   1.0)\n",
        "    w_ce_loc   = getattr(cfg, \"w_ce_loc\",   0.5)\n",
        "    w_ce_mil   = getattr(cfg, \"w_ce_mil\",   0.5)\n",
        "    w_tv_loc   = getattr(cfg, \"w_tv_loc\",   0.1)\n",
        "    w_ent_sp   = getattr(cfg, \"w_ent_sp\",   0.05)\n",
        "    w_dice     = getattr(cfg, \"w_dice\",     1.0)\n",
        "    w_ce_seg   = getattr(cfg, \"w_ce_seg\",   1.0)\n",
        "    w_cons     = getattr(cfg, \"w_cons\",     0.2)\n",
        "    w_att_ent  = getattr(cfg, \"w_att_ent\",  0.05)\n",
        "\n",
        "    # ---- Classification losses ----\n",
        "    # Fused classifier (MIL+erase + localization pooled)\n",
        "    ce_cls = F.cross_entropy(logits, y_img)\n",
        "\n",
        "    # Also ensure the localization head itself is class-discriminative\n",
        "    loc_img_logits = _pool_loc_maps_to_logits(\n",
        "        loc_maps,\n",
        "        loc_topk_ratio=getattr(cfg, \"loc_topk_ratio\", 0.2),\n",
        "        use_lse=getattr(cfg, \"loc_logsumexp\", False),\n",
        "    )\n",
        "    ce_loc = F.cross_entropy(loc_img_logits, y_img)\n",
        "\n",
        "    # Keep both MIL branches supervised\n",
        "    ce_mil = 0.5 * F.cross_entropy(logits1, y_img) + 0.5 * F.cross_entropy(logits2, y_img)\n",
        "\n",
        "    # ---- Regularizers on localization maps (no masks needed) ----\n",
        "    # Softmax over classes to get per-class spatial distributions\n",
        "    loc_soft = torch.softmax(loc_maps, dim=1)  # [B,C,H/16,W/16]\n",
        "\n",
        "    # Take the GT class channel for each sample\n",
        "    idx = torch.arange(B, device=device)\n",
        "    # shape -> [B,1,H,W], squeeze to [B, H, W]\n",
        "    p_cls = loc_soft[idx, y_img]  # class-specific prob map at /16\n",
        "\n",
        "    # Edge-aware TV to encourage smooth blobs respecting OCT edges\n",
        "    p_cls_up = F.interpolate(p_cls.unsqueeze(1), size=imgs.shape[-2:], mode='bilinear', align_corners=False)  # [B,1,H,W]\n",
        "    tv_loc = tv_edge_loss(p_cls_up, imgs, weight=0.02)\n",
        "\n",
        "    # Spatial entropy (avoid spiky single-pixel activations)\n",
        "    ent_spatial = -(p_cls * (p_cls.clamp_min(1e-8)).log()).mean()\n",
        "\n",
        "    # ---- Optional: attention entropy regularization on MIL attention ----\n",
        "    # A1: [B, C, N]; pick GT class attention over N tokens and penalize high entropy\n",
        "    att_gt = A1[idx, y_img]  # [B, N]\n",
        "    att_ent = entropy_loss(att_gt)  # normalized in your helper\n",
        "\n",
        "    # ---- Segmentation supervision (if masks available) ----\n",
        "    if has_gt_masks:\n",
        "        # One-hot for Dice\n",
        "        y_oh = F.one_hot(y_mask.clamp_max(num_classes - 1), num_classes).permute(0, 3, 1, 2).float()\n",
        "        dice = soft_dice_loss(seg_logits, y_oh)\n",
        "        ce_seg = F.cross_entropy(seg_logits, y_mask, ignore_index=255)\n",
        "    else:\n",
        "        dice = seg_logits.new_tensor(0.0)\n",
        "        ce_seg = seg_logits.new_tensor(0.0)\n",
        "\n",
        "    # ---- Consistency: decoder should agree with localization prior ----\n",
        "    # Recreate the prior at x2 resolution for comparison\n",
        "    _, _, H8, W8 = x2.shape\n",
        "    prior_up = F.interpolate(loc_maps, size=(H8, W8), mode='bilinear', align_corners=False)\n",
        "    prior_soft = torch.softmax(prior_up, dim=1)\n",
        "    seg_soft   = torch.softmax(seg_logits, dim=1)\n",
        "    prior_soft_full = F.interpolate(prior_soft, size=seg_logits.shape[-2:], mode='bilinear', align_corners=False)\n",
        "    seg_consistency = F.mse_loss(seg_soft, prior_soft_full)\n",
        "\n",
        "    # ---- Total loss ----\n",
        "    loss = (\n",
        "        w_ce_cls * ce_cls +\n",
        "        w_ce_loc * ce_loc +\n",
        "        w_ce_mil * ce_mil +\n",
        "        w_tv_loc * tv_loc +\n",
        "        w_ent_sp * ent_spatial +\n",
        "        (w_dice   * dice   if has_gt_masks else 0.0) +\n",
        "        (w_ce_seg * ce_seg if has_gt_masks else 0.0) +\n",
        "        w_cons   * seg_consistency +\n",
        "        w_att_ent * att_ent\n",
        "    )\n",
        "\n",
        "    logs = {\n",
        "        \"loss_total\":        float(loss.detach().item()),\n",
        "        \"ce_cls\":            float(ce_cls.detach().item()),\n",
        "        \"ce_loc\":            float(ce_loc.detach().item()),\n",
        "        \"ce_mil\":            float(ce_mil.detach().item()),\n",
        "        \"tv_loc\":            float(tv_loc.detach().item()),\n",
        "        \"ent_spatial\":       float(ent_spatial.detach().item()),\n",
        "        \"att_entropy\":       float(att_ent.detach().item()),\n",
        "        \"dice\":              float(dice.detach().item()) if has_gt_masks else 0.0,\n",
        "        \"ce_seg\":            float(ce_seg.detach().item()) if has_gt_masks else 0.0,\n",
        "        \"seg_consistency\":   float(seg_consistency.detach().item()),\n",
        "    }\n",
        "    return loss, logs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c9dfd22",
      "metadata": {
        "id": "9c9dfd22"
      },
      "outputs": [],
      "source": [
        "# ‚öôÔ∏è LightningModule with EMA (updated to new outputs + loss)\n",
        "from copy import deepcopy\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchmetrics.classification import Accuracy, Precision, Recall, F1Score\n",
        "from torchmetrics import ConfusionMatrix\n",
        "\n",
        "class LitV4(pl.LightningModule):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters(asdict(cfg))\n",
        "        self.cfg = cfg\n",
        "\n",
        "        # base model MUST be your updated HybridV4 (with loc_head + prior)\n",
        "        self.base = HybridV4(cfg)\n",
        "\n",
        "        # --- Metrics ---\n",
        "        self.acc  = Accuracy(task=\"multiclass\", num_classes=NUM_CLASSES)\n",
        "        self.prec = Precision(task=\"multiclass\", num_classes=NUM_CLASSES, average=\"macro\")\n",
        "        self.rec  = Recall(task=\"multiclass\", num_classes=NUM_CLASSES, average=\"macro\")\n",
        "        self.f1   = F1Score(task=\"multiclass\", num_classes=NUM_CLASSES, average=\"macro\")\n",
        "        self.cm   = ConfusionMatrix(task=\"multiclass\", num_classes=NUM_CLASSES)\n",
        "\n",
        "        # --- Graph cache ---\n",
        "        self.A_hat = None\n",
        "        self.coords = None\n",
        "\n",
        "        # --- EMA over *base* (HybridV4) only ---\n",
        "        self.ema_model = None\n",
        "\n",
        "    def _get_graph(self, device):\n",
        "        if (self.A_hat is None) or (self.A_hat.device != device):\n",
        "            self.A_hat, self.coords = build_grid(self.cfg.patch_rows, self.cfg.patch_cols, device)\n",
        "        return self.A_hat, self.coords\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        opt = torch.optim.AdamW(self.parameters(), lr=self.cfg.lr, weight_decay=self.cfg.weight_decay)\n",
        "        # keep your plateau scheduler (monitor must exist; see notes below)\n",
        "        sch = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            opt, mode=\"max\", factor=0.5, patience=2\n",
        "        )\n",
        "        return {\"optimizer\": opt, \"lr_scheduler\": {\"scheduler\": sch, \"monitor\": self.cfg.monitor}}\n",
        "\n",
        "    def on_train_start(self):\n",
        "        # Create a non-trainable EMA copy of the *base* model (HybridV4)\n",
        "        self.ema_model = deepcopy(self.base).to(self.device)\n",
        "        for p in self.ema_model.parameters():\n",
        "            p.requires_grad_(False)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _ema_update(self):\n",
        "        if self.ema_model is None:\n",
        "            return\n",
        "        d = self.cfg.ema_decay\n",
        "        for p, q in zip(self.base.parameters(), self.ema_model.parameters()):\n",
        "            q.data.mul_(d).add_(p.data * (1.0 - d))\n",
        "\n",
        "    def forward(self, x, A_hat, coords):\n",
        "        return self.base(x, A_hat, coords)\n",
        "\n",
        "    # -------- shared train/val/test step ----------\n",
        "    def _shared(self, batch, stage: str):\n",
        "        imgs = batch[\"image\"].to(self.device)\n",
        "        y    = batch[\"label\"].to(self.device)\n",
        "\n",
        "        A_hat, coords = self._get_graph(imgs.device)\n",
        "\n",
        "        # EXPECTED: (log1,A1),(log2,A2),loc_maps,logits,Xg,x2,x3,seg_logits\n",
        "        outputs = self.base(imgs, A_hat, coords)\n",
        "\n",
        "        # New total loss (uses loc_maps + fused logits + decoder)\n",
        "        loss, logs = compute_total_loss(outputs, batch, self.cfg, use_masks=(\"mask\" in batch))\n",
        "\n",
        "        # classification metrics from fused logits\n",
        "        logits = outputs[3]\n",
        "        preds  = logits.argmax(dim=1)\n",
        "        self.acc.update(preds, y)\n",
        "        self.prec.update(preds, y)\n",
        "        self.rec.update(preds, y)\n",
        "        self.f1.update(preds, y)\n",
        "        if stage != \"train\":\n",
        "            self.cm.update(preds, y)\n",
        "\n",
        "        # log scalar loss + key terms\n",
        "        self.log(f\"{stage}/loss\", loss.detach(), prog_bar=True, on_epoch=True)\n",
        "        # also log a monitor-friendly accuracy for scheduler / checkpointing\n",
        "        cls_acc = (preds == y).float().mean()\n",
        "        self.log(f\"{stage}/cls_acc\", cls_acc, prog_bar=True, on_epoch=True)\n",
        "\n",
        "        for k, v in logs.items():\n",
        "            self.log(f\"{stage}/{k}\", v, prog_bar=False, on_epoch=True)\n",
        "\n",
        "        if stage == \"train\":\n",
        "            self._ema_update()\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        return self._shared(batch, \"train\")\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        self._shared(batch, \"val\")\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        self._shared(batch, \"test\")\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        acc  = self.acc.compute().item()\n",
        "        prec = self.prec.compute().item()\n",
        "        rec  = self.rec.compute().item()\n",
        "        f1   = self.f1.compute().item()\n",
        "        self.log(\"val/cls_acc\",  acc,  prog_bar=True)\n",
        "        self.log(\"val/cls_prec\", prec)\n",
        "        self.log(\"val/cls_rec\",  rec)\n",
        "        self.log(\"val/cls_f1\",   f1,   prog_bar=True)\n",
        "        self.acc.reset(); self.prec.reset(); self.rec.reset(); self.f1.reset(); self.cm.reset()\n",
        "\n",
        "    def on_test_epoch_end(self):\n",
        "        acc  = self.acc.compute().item()\n",
        "        prec = self.prec.compute().item()\n",
        "        rec  = self.rec.compute().item()\n",
        "        f1   = self.f1.compute().item()\n",
        "        console.rule(\"[bold]Test Classification Metrics[/bold]\")\n",
        "        table = Table(\"Metric\",\"Value\")\n",
        "        for k,v in [(\"Accuracy\",acc),(\"Precision\",prec),(\"Recall\",rec),(\"F1\",f1)]:\n",
        "            table.add_row(k, f\"{v:.4f}\")\n",
        "        console.print(table); console.print(\"CM\")\n",
        "        self.acc.reset(); self.prec.reset(); self.rec.reset(); self.f1.reset(); self.cm.reset()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a2f8502",
      "metadata": {
        "cellView": "form",
        "id": "0a2f8502"
      },
      "outputs": [],
      "source": [
        "# @title üñºÔ∏è Visualization (updated)\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def overlay(img_t, heat, alpha=0.45):\n",
        "    \"\"\"\n",
        "    img_t: [1,H,W] tensor (normalized by cfg.mean/std)\n",
        "    heat:  [H,W] float in [0,1]\n",
        "    \"\"\"\n",
        "    img = img_t.detach().cpu().numpy().squeeze()\n",
        "    img = np.clip(img * cfg.normalize_std + cfg.normalize_mean, 0, 1)\n",
        "    img_rgb = np.dstack([img]*3)\n",
        "    return show_cam_on_image(img_rgb, heat, use_rgb=True,\n",
        "                             colormap=cv2.COLORMAP_JET, image_weight=1-alpha)\n",
        "\n",
        "def _normalize01(arr):\n",
        "    arr = arr.astype(np.float32)\n",
        "    mn, mx = float(arr.min()), float(arr.max())\n",
        "    if mx - mn < 1e-12:\n",
        "        return np.zeros_like(arr, dtype=np.float32)\n",
        "    return (arr - mn) / (mx - mn + 1e-12)\n",
        "\n",
        "def postprocess_prob(prob01):\n",
        "    \"\"\"\n",
        "    prob01: [H,W] in [0,1]\n",
        "    -> returns binary mask [H,W] {0,1}:\n",
        "       blur -> Otsu threshold -> open/close -> largest CC\n",
        "    \"\"\"\n",
        "    H, W = prob01.shape\n",
        "    prob = (prob01 * 255.0).astype(np.uint8)\n",
        "    prob = cv2.GaussianBlur(prob, (5,5), 0)\n",
        "\n",
        "    # Otsu; if very flat, fall back to percentile\n",
        "    _, mask = cv2.threshold(prob, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "    if mask.sum() < 10:  # too small ‚Üí percentile fallback\n",
        "        thr = np.percentile(prob01, 70.0)\n",
        "        mask = (prob01 >= thr).astype(np.uint8) * 255\n",
        "    else:\n",
        "        mask = (mask > 0).astype(np.uint8) * 255\n",
        "\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, np.ones((3,3), np.uint8))\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, np.ones((5,5), np.uint8))\n",
        "\n",
        "    # keep largest connected component (if any)\n",
        "    num, cc, stats, _ = cv2.connectedComponentsWithStats(mask, connectivity=8)\n",
        "    if num > 1:\n",
        "        areas = stats[1:, cv2.CC_STAT_AREA]\n",
        "        k = 1 + int(np.argmax(areas))\n",
        "        mask = (cc == k).astype(np.uint8) * 255\n",
        "    return (mask > 0).astype(np.uint8)\n",
        "\n",
        "def plot_batch(lit, batch, nshow=4, show_mil=False):\n",
        "    \"\"\"\n",
        "    lit.base must return:\n",
        "      (log1,A1),(log2,A2),loc_maps,logits,Xg,x2,x3,seg_logits\n",
        "    \"\"\"\n",
        "    lit.eval()\n",
        "    device = lit.device\n",
        "    A_hat, coords = lit._get_graph(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        imgs = batch[\"image\"].to(device)                      # [B,1,H,W]\n",
        "        (log1, A1), (log2, A2), loc_maps, logits, Xg, x2, x3, seg_logits = lit.base(imgs, A_hat, coords)\n",
        "\n",
        "    B, _, H, W = imgs.shape\n",
        "    n = min(nshow, B)\n",
        "\n",
        "    # columns: Input | Loc head | Decoder | Postprocessed\n",
        "    ncols = 4 if not show_mil else 5\n",
        "    plt.figure(figsize=(4*ncols, 3*n))\n",
        "\n",
        "    # derive token grid for MIL attention reshaping from x3\n",
        "    _, _, H16, W16 = x3.shape\n",
        "\n",
        "    for i in range(n):\n",
        "        pred = int(logits[i].argmax().item())\n",
        "\n",
        "        # --- Localization head ---\n",
        "        loc = loc_maps[i, pred].detach().cpu().numpy()          # [H/16, W/16]\n",
        "        loc = _normalize01(loc)\n",
        "        loc_up = cv2.resize(loc, (W, H), interpolation=cv2.INTER_LINEAR)\n",
        "        over_loc = overlay(imgs[i], loc_up, 0.45)\n",
        "\n",
        "        # --- Decoder prob for predicted class ---\n",
        "        dec = torch.softmax(seg_logits, dim=1)[i, pred].detach().cpu().numpy()\n",
        "        dec = _normalize01(dec)\n",
        "        over_dec = overlay(imgs[i], dec, 0.45)\n",
        "\n",
        "        # --- Postprocess (simple mask from localization head; you can also fuse with dec) ---\n",
        "        mask_loc = postprocess_prob(loc_up)\n",
        "        # (optional) fuse masks: mask = ((mask_loc + (dec > 0.5)) > 0).astype(np.uint8)\n",
        "        mask_rgb = np.dstack([mask_loc*255]*3)\n",
        "\n",
        "        # --- (Optional) MIL attention for comparison ---\n",
        "        if show_mil:\n",
        "            att = A1[i, pred].view(H16, W16).detach().cpu().numpy()\n",
        "            att = _normalize01(att)\n",
        "            att_up = cv2.resize(att, (W, H), interpolation=cv2.INTER_LINEAR)\n",
        "            over_att = overlay(imgs[i], att_up, 0.45)\n",
        "\n",
        "        # ---- Plot row ----\n",
        "        # 1) Input\n",
        "        plt.subplot(n, ncols, i*ncols + 1)\n",
        "        plt.imshow(imgs[i].detach().cpu().squeeze(), cmap='gray')\n",
        "        plt.axis('off'); plt.title(\"Input\")\n",
        "\n",
        "        # 2) Localization head\n",
        "        plt.subplot(n, ncols, i*ncols + 2)\n",
        "        plt.imshow(over_loc)\n",
        "        plt.axis('off'); plt.title(f\"Localization head | Pred: {CLASS_NAMES[pred]}\")\n",
        "\n",
        "        # 3) Decoder\n",
        "        plt.subplot(n, ncols, i*ncols + 3)\n",
        "        plt.imshow(over_dec)\n",
        "        plt.axis('off'); plt.title(\"Decoder (pred class)\")\n",
        "\n",
        "        # 4) Postprocessed mask (from loc head)\n",
        "        plt.subplot(n, ncols, i*ncols + 4)\n",
        "        # Blend mask over input for visibility\n",
        "        base = imgs[i].detach().cpu().squeeze().numpy()\n",
        "        base = np.clip(base * cfg.normalize_std + cfg.normalize_mean, 0, 1)\n",
        "        base_rgb = (np.dstack([base]*3) * 255).astype(np.uint8)\n",
        "        blend = base_rgb.copy()\n",
        "        blend[mask_loc.astype(bool)] = (0.6*blend[mask_loc.astype(bool)] + 0.4*np.array([255, 64, 64])).astype(np.uint8)\n",
        "        plt.imshow(blend)\n",
        "        plt.axis('off'); plt.title(\"Postprocessed mask\")\n",
        "\n",
        "        # 5) (Optional) MIL attention\n",
        "        if show_mil:\n",
        "            plt.subplot(n, ncols, i*ncols + 5)\n",
        "            plt.imshow(over_att)\n",
        "            plt.axis('off'); plt.title(\"MIL attention (pred class)\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22834240",
      "metadata": {
        "id": "22834240"
      },
      "outputs": [],
      "source": [
        "# üöÄ Train / Test / Visualize (final)\n",
        "assert cfg.data_root != \"\", \"cfg.data_root is empty\"\n",
        "\n",
        "dm  = OCTDataModule(cfg); dm.setup()\n",
        "lit = LitV4(cfg)\n",
        "\n",
        "logger = TensorBoardLogger(save_dir=\"tb_logs\", name=cfg.project, version=cfg.run_name)\n",
        "# IMPORTANT: monitor a metric you log; here we use val/cls_acc\n",
        "ckpt  = ModelCheckpoint(monitor=\"val/cls_acc\", mode=\"max\", save_top_k=cfg.save_top_k,\n",
        "                        filename=\"{epoch}-{val_cls_acc:.4f}\")\n",
        "early = EarlyStopping(monitor=\"val/cls_acc\", mode=\"max\", patience=4)\n",
        "lrm   = LearningRateMonitor(logging_interval=\"epoch\")\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=cfg.max_epochs,\n",
        "    precision=cfg.precision,\n",
        "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
        "    devices=1,\n",
        "    logger=logger,\n",
        "    callbacks=[ckpt, early, lrm],\n",
        "    log_every_n_steps=20\n",
        ")\n",
        "\n",
        "trainer.fit(lit, dm)\n",
        "trainer.test(lit, datamodule=dm, ckpt_path=\"best\")\n",
        "\n",
        "# Visualize a test batch with the UPDATED plot_batch() you pasted earlier\n",
        "batch = next(iter(dm.test_dataloader()))\n",
        "plot_batch(lit, batch, nshow=4, show_mil=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()   # will prompt you to select a file from your laptop"
      ],
      "metadata": {
        "id": "GyMewmJCYhrG"
      },
      "id": "GyMewmJCYhrG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import torch\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Preprocessing (must match your training!)\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.Resize((cfg.img_size, cfg.img_size)),   # same size you trained with\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[cfg.normalize_mean], std=[cfg.normalize_std]),\n",
        "])\n",
        "\n",
        "def run_custom_inference(img_path, lit, device=\"cuda\"):\n",
        "    lit.eval().to(device)\n",
        "\n",
        "    # Load + preprocess\n",
        "    img_pil = Image.open(img_path).convert(\"L\")\n",
        "    x = preprocess(img_pil).unsqueeze(0).to(device)  # [1,1,H,W]\n",
        "\n",
        "    # Graph\n",
        "    A_hat, coords = lit._get_graph(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        (log1,A1),(log2,A2),loc_maps,logits,Xg,x2,x3,seg_logits = lit.base(x, A_hat, coords)\n",
        "\n",
        "    pred_class = logits.argmax(1).item()\n",
        "\n",
        "    # --- Heatmap from localization head ---\n",
        "    loc = loc_maps[0, pred_class].cpu().numpy()\n",
        "    loc = (loc - loc.min()) / (loc.max() + 1e-6)\n",
        "    loc_up = cv2.resize(loc, (x.shape[-1], x.shape[-2]), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "    # --- Heatmap from decoder ---\n",
        "    dec = torch.softmax(seg_logits, dim=1)[0, pred_class].cpu().numpy()\n",
        "    dec = (dec - dec.min()) / (dec.max() + 1e-6)\n",
        "\n",
        "    return img_pil, loc_up, dec, pred_class\n"
      ],
      "metadata": {
        "id": "huyIZ9-eYnKQ"
      },
      "id": "huyIZ9-eYnKQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If you uploaded directly:\n",
        "img_path = \"/content/Screenshot 2025-08-30 155818.png\"\n",
        "\n",
        "# If on Google Drive:\n",
        "# img_path = \"/content/drive/MyDrive/OCT_papers/sample1.png\"\n",
        "\n",
        "img_pil, loc_map, dec_map, pred_class = run_custom_inference(img_path, lit)\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.subplot(1,3,1); plt.imshow(img_pil, cmap=\"gray\"); plt.axis(\"off\"); plt.title(\"Input OCT\")\n",
        "plt.subplot(1,3,2); plt.imshow(img_pil, cmap=\"gray\"); plt.imshow(loc_map, cmap=\"jet\", alpha=0.45); plt.axis(\"off\"); plt.title(\"Localization head heatmap\")\n",
        "plt.subplot(1,3,3); plt.imshow(img_pil, cmap=\"gray\"); plt.imshow(dec_map, cmap=\"jet\", alpha=0.45); plt.axis(\"off\"); plt.title(\"Decoder heatmap\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QBJSTTrsZi8-"
      },
      "id": "QBJSTTrsZi8-",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}